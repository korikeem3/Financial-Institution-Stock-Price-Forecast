{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "id": "5tcDWB3X3dou",
        "outputId": "f5e16c6e-51cb-4343-c1fe-6c8530489510"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-29bd2a7d-7990-478b-9a62-8f11b36ccbb2\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-29bd2a7d-7990-478b-9a62-8f11b36ccbb2\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving JPM.csv to JPM.csv\n",
            "Saving Goldman.csv to Goldman.csv\n",
            "Saving BofA.csv to BofA.csv\n",
            "Saving Rates.csv to Rates.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.stattools import adfuller, grangercausalitytests\n",
        "import seaborn as sns\n",
        "import os\n",
        "import warnings\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "# only need to run this cell once to install packages and install CSV files\n",
        "# check files to see if BofA.csv, Goldman.csv, JPM.csv, and Rates.csv are present\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this cell will run all analysis print out accuracy metrics\n",
        "# graphs will be saved as pngs in files on the left with CSV files\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def parse_number(x):\n",
        "    if isinstance(x, str):\n",
        "        x = x.replace(',', '').replace('$', '').replace('%', '').strip()\n",
        "        if x.startswith('(') and x.endswith(')'):\n",
        "            x = '-' + x[1:-1]\n",
        "        try:\n",
        "            return float(x)\n",
        "        except ValueError:\n",
        "            return np.nan\n",
        "    elif isinstance(x, (int, float)):\n",
        "        return x\n",
        "    else:\n",
        "        return np.nan\n",
        "\n",
        "# Load and prepare bank data\n",
        "def load_bank_data(filename):\n",
        "    try:\n",
        "        df = pd.read_csv(filename, thousands=',')\n",
        "        print(f\"Successfully read {filename}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File not found: {filename}\")\n",
        "        return None\n",
        "    except pd.errors.ParserError as e:\n",
        "        print(f\"Parser error reading {filename}: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error reading {filename}: {e}\")\n",
        "        return None\n",
        "\n",
        "    df.columns = df.columns.str.strip()\n",
        "    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
        "\n",
        "    if 'Pricing Date' not in df.columns:\n",
        "        print(\"Column 'Pricing Date' not found in the DataFrame.\")\n",
        "        print(\"Available columns:\", df.columns.tolist())\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        df['Pricing Date'] = pd.to_datetime(df['Pricing Date'], format='%m-%d-%Y', errors='coerce')\n",
        "        if df['Pricing Date'].isnull().any():\n",
        "            print(\"Some 'Pricing Date' values could not be converted to datetime.\")\n",
        "            df = df.dropna(subset=['Pricing Date'])\n",
        "    except Exception as e:\n",
        "        print(f\"Error converting 'Pricing Date' to datetime: {e}\")\n",
        "        return None\n",
        "\n",
        "    if df.empty:\n",
        "        print(f\"All 'Pricing Date' values in {filename} could not be converted. Skipping this file.\")\n",
        "        return None\n",
        "\n",
        "    df = df.set_index('Pricing Date')\n",
        "    df.index = pd.to_datetime(df.index)\n",
        "    df = df.sort_index()\n",
        "\n",
        "    # Handle duplicate dates\n",
        "    if df.index.duplicated().any():\n",
        "        print(\"Duplicate dates found in index. Handling duplicates...\")\n",
        "        df = df[~df.index.duplicated(keep='first')]  # Remove duplicates\n",
        "\n",
        "    df = df.asfreq('B')\n",
        "    df = df.ffill()\n",
        "    return df\n",
        "\n",
        "def load_rates_data(filename):\n",
        "    try:\n",
        "        df = pd.read_csv(filename, thousands=',')\n",
        "        print(f\"Successfully read {filename}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File not found: {filename}\")\n",
        "        return None\n",
        "    except pd.errors.ParserError as e:\n",
        "        print(f\"Parser error reading {filename}: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error reading {filename}: {e}\")\n",
        "        return None\n",
        "\n",
        "    df.columns = df.columns.str.strip()\n",
        "\n",
        "    if 'DATE' not in df.columns:\n",
        "        print(\"Column 'DATE' not found in the DataFrame.\")\n",
        "        print(\"Available columns:\", df.columns.tolist())\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        df['DATE'] = pd.to_datetime(df['DATE'], errors='coerce')\n",
        "        if df['DATE'].isnull().any():\n",
        "            print(\"Some 'DATE' values could not be converted to datetime.\")\n",
        "            df = df.dropna(subset=['DATE'])\n",
        "    except Exception as e:\n",
        "        print(f\"Error converting 'DATE' to datetime: {e}\")\n",
        "        return None\n",
        "\n",
        "    if df.empty:\n",
        "        print(f\"All 'DATE' values in {filename} could not be converted. Skipping this file.\")\n",
        "        return None\n",
        "\n",
        "    df = df.set_index('DATE')\n",
        "    df.index = pd.to_datetime(df.index)\n",
        "    df = df.sort_index()\n",
        "    df = df.asfreq('B')\n",
        "    df = df.ffill()\n",
        "    return df\n",
        "\n",
        "# Perform and print ADF test results\n",
        "def perform_stationarity_test(series, name):\n",
        "    print(f\"\\n=== Stationarity Test for {name} ===\")\n",
        "    result = adfuller(series.dropna())\n",
        "    print('ADF Statistic:', result[0])\n",
        "    print('p-value:', result[1])\n",
        "    print('Critical values:')\n",
        "    for key, value in result[4].items():\n",
        "        print(f'\\t{key}: {value}')\n",
        "    is_stationary = result[1] < 0.05\n",
        "    if is_stationary:\n",
        "        print(f\"{name} is stationary.\")\n",
        "    else:\n",
        "        print(f\"{name} is non-stationary. Differencing is recommended.\")\n",
        "    return is_stationary\n",
        "\n",
        "\n",
        "def extract_granger_pvalues(granger_result):\n",
        "    p_values = {}\n",
        "    for lag in granger_result.keys():\n",
        "        test_result = granger_result[lag][0]['ssr_chi2test']\n",
        "        p_values[lag] = test_result[1]\n",
        "    return p_values\n",
        "def analyze_bank_rates_relationship(bank_df, rates_df, bank_name):\n",
        "    if bank_df is None or rates_df is None:\n",
        "        print(f\"Data for {bank_name} or interest rates is missing.\")\n",
        "        return None\n",
        "\n",
        "    bank_df = bank_df.loc[:, ~bank_df.columns.str.contains('^Unnamed')]\n",
        "\n",
        "    # Merge data\n",
        "    merged_df = pd.merge(bank_df, rates_df, left_index=True, right_index=True, how='inner')\n",
        "    merged_df = merged_df.dropna()\n",
        "\n",
        "    if merged_df.empty:\n",
        "        print(f\"No overlapping dates between {bank_name} data and interest rates.\")\n",
        "        return None\n",
        "\n",
        "    # Get price column name\n",
        "    price_col_candidates = [col for col in merged_df.columns if 'Share Price' in col]\n",
        "    if not price_col_candidates:\n",
        "        print(f\"No 'Share Price' column found for {bank_name}.\")\n",
        "        print(\"Available columns:\", merged_df.columns.tolist())\n",
        "        return None\n",
        "    price_col = price_col_candidates[0]\n",
        "\n",
        "    merged_df[price_col] = merged_df[price_col].apply(parse_number).astype(float)\n",
        "\n",
        "    if 'MORTGAGE30US' not in merged_df.columns:\n",
        "        print(\"'MORTGAGE30US' column not found in interest rate data.\")\n",
        "        print(\"Available columns in merged_df:\", merged_df.columns.tolist())\n",
        "        return None\n",
        "\n",
        "    merged_df['MORTGAGE30US'] = merged_df['MORTGAGE30US'].apply(parse_number).astype(float)\n",
        "\n",
        "    # Drop any rows with NaNs resulted from conversion\n",
        "    merged_df = merged_df.dropna(subset=[price_col, 'MORTGAGE30US'])\n",
        "    merged_df = merged_df.replace([np.inf, -np.inf], np.nan)\n",
        "    merged_df = merged_df.dropna(subset=[price_col, 'MORTGAGE30US'])\n",
        "\n",
        "    if merged_df.shape[0] < 10:\n",
        "        print(f\"Not enough data points after cleaning for {bank_name}.\")\n",
        "        return None\n",
        "\n",
        "    print(f\"Date range of merged_df: {merged_df.index.min()} to {merged_df.index.max()}\")\n",
        "\n",
        "    # Calculate correlation\n",
        "    correlation = merged_df[price_col].corr(merged_df['MORTGAGE30US'])\n",
        "    print(f\"Correlation between {price_col} and MORTGAGE30US: {correlation}\")\n",
        "\n",
        "    # Perform Granger causality test\n",
        "    num_obs = merged_df.shape[0]\n",
        "    max_lag = min(5, num_obs - 2)\n",
        "    if max_lag < 1:\n",
        "        print(f\"Insufficient observations for Granger causality test for {bank_name}.\")\n",
        "        granger_pvalues = None\n",
        "    else:\n",
        "        try:\n",
        "            granger_result = grangercausalitytests(\n",
        "                merged_df[[price_col, 'MORTGAGE30US']], maxlag=max_lag, verbose=False\n",
        "            )\n",
        "            granger_pvalues = extract_granger_pvalues(granger_result)\n",
        "        except Exception as e:\n",
        "            print(f\"Error performing Granger causality test for {bank_name}: {e}\")\n",
        "            granger_pvalues = None\n",
        "\n",
        "    # Store original series before differencing\n",
        "    original_price_series = merged_df[price_col].copy()\n",
        "    original_rate_series = merged_df['MORTGAGE30US'].copy()\n",
        "\n",
        "    # Check for stationarity\n",
        "    is_stationary_price = perform_stationarity_test(merged_df[price_col], f\"{bank_name} Price\")\n",
        "    is_stationary_rate = perform_stationarity_test(merged_df['MORTGAGE30US'], \"Interest Rate\")\n",
        "\n",
        "    # Initialize differencing order\n",
        "    d_price = 0\n",
        "    d_rate = 0\n",
        "\n",
        "    # Apply differencing if necessary\n",
        "    if not is_stationary_price:\n",
        "        merged_df[price_col] = merged_df[price_col].diff()\n",
        "        d_price += 1\n",
        "\n",
        "    if not is_stationary_rate:\n",
        "        merged_df['MORTGAGE30US'] = merged_df['MORTGAGE30US'].diff()\n",
        "        d_rate += 1\n",
        "\n",
        "    # Re-drop NA after differencing\n",
        "    merged_df = merged_df.dropna()\n",
        "\n",
        "    # Re-test for stationarity after differencing\n",
        "    if not is_stationary_price:\n",
        "        is_stationary_price = perform_stationarity_test(merged_df[price_col], f\"{bank_name} Price After Differencing\")\n",
        "    if not is_stationary_rate:\n",
        "        is_stationary_rate = perform_stationarity_test(merged_df['MORTGAGE30US'], \"Interest Rate After Differencing\")\n",
        "\n",
        "    if not is_stationary_price or not is_stationary_rate:\n",
        "        print(f\"Series are still non-stationary after differencing for {bank_name}.\")\n",
        "        return None\n",
        "\n",
        "    # Split data into training and testing sets\n",
        "    train_size = int(len(merged_df) * 0.8)\n",
        "    train_data = merged_df.iloc[:train_size]\n",
        "    test_data = merged_df.iloc[train_size:]\n",
        "\n",
        "    forecast_steps = len(test_data)\n",
        "    actual = original_price_series.iloc[train_size:train_size + forecast_steps]\n",
        "\n",
        "    # ARIMA Model with Exogenous Variables\n",
        "    arima_order = (2, d_price, 2)\n",
        "    arima_model = ARIMA(\n",
        "        train_data[price_col],\n",
        "        exog=train_data[['MORTGAGE30US']],\n",
        "        order=arima_order\n",
        "    )\n",
        "    try:\n",
        "        arima_results = arima_model.fit()\n",
        "    except Exception as e:\n",
        "        print(f\"Error fitting ARIMA model for {bank_name}: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Forecasting with ARIMA (with exogenous variables)\n",
        "    try:\n",
        "        arima_forecast = arima_results.get_forecast(steps=forecast_steps, exog=test_data[['MORTGAGE30US']])\n",
        "        arima_forecast_mean = arima_forecast.predicted_mean\n",
        "        arima_conf_int = arima_forecast.conf_int()\n",
        "        arima_conf_int.columns = ['lower', 'upper']\n",
        "        arima_forecast_mean.index = test_data.index\n",
        "        arima_conf_int.index = test_data.index\n",
        "\n",
        "        # Reconstruct forecasted levels if differenced\n",
        "        if d_price > 0:\n",
        "            last_value = original_price_series.iloc[train_size - 1]\n",
        "            arima_forecast_values = arima_forecast_mean.cumsum() + last_value\n",
        "            arima_conf_int['lower'] = arima_conf_int['lower'].cumsum() + last_value\n",
        "            arima_conf_int['upper'] = arima_conf_int['upper'].cumsum() + last_value\n",
        "            arima_forecast_mean = arima_forecast_values\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error forecasting with ARIMA for {bank_name}: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Calculate ARIMA accuracy metrics (with exogenous variables)\n",
        "    arima_mae = np.mean(np.abs(actual - arima_forecast_mean))\n",
        "    arima_mse = np.mean((actual - arima_forecast_mean) ** 2)\n",
        "    arima_rmse = np.sqrt(arima_mse)\n",
        "    arima_mape = np.mean(np.abs((actual - arima_forecast_mean) / actual)) * 100\n",
        "\n",
        "    arima_accuracy_metrics = {\n",
        "        'ARIMA_MAE_with_exog': arima_mae,\n",
        "        'ARIMA_MSE_with_exog': arima_mse,\n",
        "        'ARIMA_RMSE_with_exog': arima_rmse,\n",
        "        'ARIMA_MAPE_with_exog': arima_mape\n",
        "    }\n",
        "\n",
        "    # ARIMA Model without Exogenous Variables\n",
        "    arima_model_no_exog = ARIMA(\n",
        "        train_data[price_col],\n",
        "        order=arima_order\n",
        "    )\n",
        "    try:\n",
        "        arima_results_no_exog = arima_model_no_exog.fit()\n",
        "    except Exception as e:\n",
        "        print(f\"Error fitting ARIMA model without exogenous variables for {bank_name}: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Forecasting with ARIMA (without exogenous variables)\n",
        "    try:\n",
        "        arima_forecast_no_exog = arima_results_no_exog.get_forecast(steps=forecast_steps)\n",
        "        arima_forecast_mean_no_exog = arima_forecast_no_exog.predicted_mean\n",
        "        arima_conf_int_no_exog = arima_forecast_no_exog.conf_int()\n",
        "        arima_conf_int_no_exog.columns = ['lower', 'upper']\n",
        "        arima_forecast_mean_no_exog.index = test_data.index\n",
        "        arima_conf_int_no_exog.index = test_data.index\n",
        "\n",
        "        # Reconstruct forecasted levels if differenced\n",
        "        if d_price > 0:\n",
        "            last_value = original_price_series.iloc[train_size - 1]\n",
        "            arima_forecast_values_no_exog = arima_forecast_mean_no_exog.cumsum() + last_value\n",
        "            arima_conf_int_no_exog['lower'] = arima_conf_int_no_exog['lower'].cumsum() + last_value\n",
        "            arima_conf_int_no_exog['upper'] = arima_conf_int_no_exog['upper'].cumsum() + last_value\n",
        "            arima_forecast_mean_no_exog = arima_forecast_values_no_exog\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error forecasting with ARIMA without exogenous variables for {bank_name}: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Calculate ARIMA accuracy metrics (without exogenous variables)\n",
        "    arima_mae_no_exog = np.mean(np.abs(actual - arima_forecast_mean_no_exog))\n",
        "    arima_mse_no_exog = np.mean((actual - arima_forecast_mean_no_exog) ** 2)\n",
        "    arima_rmse_no_exog = np.sqrt(arima_mse_no_exog)\n",
        "    arima_mape_no_exog = np.mean(np.abs((actual - arima_forecast_mean_no_exog) / actual)) * 100\n",
        "\n",
        "    arima_accuracy_metrics_no_exog = {\n",
        "        'ARIMA_MAE_no_exog': arima_mae_no_exog,\n",
        "        'ARIMA_MSE_no_exog': arima_mse_no_exog,\n",
        "        'ARIMA_RMSE_no_exog': arima_rmse_no_exog,\n",
        "        'ARIMA_MAPE_no_exog': arima_mape_no_exog\n",
        "    }\n",
        "\n",
        "    # SARIMA Model with Exogenous Variables\n",
        "    seasonal_order = (1, 0, 1, 5)\n",
        "    sarima_model = SARIMAX(\n",
        "        train_data[price_col],\n",
        "        exog=train_data[['MORTGAGE30US']],\n",
        "        order=(2, d_price, 2),\n",
        "        seasonal_order=seasonal_order,\n",
        "        enforce_stationarity=False,\n",
        "        enforce_invertibility=False\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        sarima_results = sarima_model.fit(disp=False)\n",
        "    except Exception as e:\n",
        "        print(f\"Error fitting SARIMA model for {bank_name}: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Forecasting with SARIMA (with exogenous variables)\n",
        "    try:\n",
        "        sarima_forecast = sarima_results.get_forecast(steps=forecast_steps, exog=test_data[['MORTGAGE30US']])\n",
        "        sarima_forecast_mean = sarima_forecast.predicted_mean\n",
        "        sarima_conf_int = sarima_forecast.conf_int()\n",
        "        sarima_conf_int.columns = ['lower', 'upper']\n",
        "        sarima_forecast_mean.index = test_data.index\n",
        "        sarima_conf_int.index = test_data.index\n",
        "\n",
        "        # Reconstruct forecasted levels if differenced\n",
        "        if d_price > 0:\n",
        "            last_value = original_price_series.iloc[train_size - 1]\n",
        "            sarima_forecast_values = sarima_forecast_mean.cumsum() + last_value\n",
        "            sarima_conf_int['lower'] = sarima_conf_int['lower'].cumsum() + last_value\n",
        "            sarima_conf_int['upper'] = sarima_conf_int['upper'].cumsum() + last_value\n",
        "            sarima_forecast_mean = sarima_forecast_values\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error forecasting with SARIMA for {bank_name}: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Calculate SARIMA accuracy metrics (with exogenous variables)\n",
        "    sarima_mae = np.mean(np.abs(actual - sarima_forecast_mean))\n",
        "    sarima_mse = np.mean((actual - sarima_forecast_mean) ** 2)\n",
        "    sarima_rmse = np.sqrt(sarima_mse)\n",
        "    sarima_mape = np.mean(np.abs((actual - sarima_forecast_mean) / actual)) * 100\n",
        "\n",
        "    sarima_accuracy_metrics = {\n",
        "        'SARIMA_MAE_with_exog': sarima_mae,\n",
        "        'SARIMA_MSE_with_exog': sarima_mse,\n",
        "        'SARIMA_RMSE_with_exog': sarima_rmse,\n",
        "        'SARIMA_MAPE_with_exog': sarima_mape\n",
        "    }\n",
        "\n",
        "    # SARIMA Model without Exogenous Variables\n",
        "    sarima_model_no_exog = SARIMAX(\n",
        "        train_data[price_col],\n",
        "        order=(1, d_price, 1),\n",
        "        seasonal_order=seasonal_order,\n",
        "        enforce_stationarity=False,\n",
        "        enforce_invertibility=False\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        sarima_results_no_exog = sarima_model_no_exog.fit(disp=False)\n",
        "    except Exception as e:\n",
        "        print(f\"Error fitting SARIMA model without exogenous variables for {bank_name}: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Forecasting with SARIMA (without exogenous variables)\n",
        "    try:\n",
        "        sarima_forecast_no_exog = sarima_results_no_exog.get_forecast(steps=forecast_steps)\n",
        "        sarima_forecast_mean_no_exog = sarima_forecast_no_exog.predicted_mean\n",
        "        sarima_conf_int_no_exog = sarima_forecast_no_exog.conf_int()\n",
        "        sarima_conf_int_no_exog.columns = ['lower', 'upper']\n",
        "        sarima_forecast_mean_no_exog.index = test_data.index\n",
        "        sarima_conf_int_no_exog.index = test_data.index\n",
        "\n",
        "        # Reconstruct forecasted levels if differenced\n",
        "        if d_price > 0:\n",
        "            last_value = original_price_series.iloc[train_size - 1]\n",
        "            sarima_forecast_values_no_exog = sarima_forecast_mean_no_exog.cumsum() + last_value\n",
        "            sarima_conf_int_no_exog['lower'] = sarima_conf_int_no_exog['lower'].cumsum() + last_value\n",
        "            sarima_conf_int_no_exog['upper'] = sarima_conf_int_no_exog['upper'].cumsum() + last_value\n",
        "            sarima_forecast_mean_no_exog = sarima_forecast_values_no_exog\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error forecasting with SARIMA without exogenous variables for {bank_name}: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Calculate SARIMA accuracy metrics (without exogenous variables)\n",
        "    sarima_mae_no_exog = np.mean(np.abs(actual - sarima_forecast_mean_no_exog))\n",
        "    sarima_mse_no_exog = np.mean((actual - sarima_forecast_mean_no_exog) ** 2)\n",
        "    sarima_rmse_no_exog = np.sqrt(sarima_mse_no_exog)\n",
        "    sarima_mape_no_exog = np.mean(np.abs((actual - sarima_forecast_mean_no_exog) / actual)) * 100\n",
        "\n",
        "    sarima_accuracy_metrics_no_exog = {\n",
        "        'SARIMA_MAE_no_exog': sarima_mae_no_exog,\n",
        "        'SARIMA_MSE_no_exog': sarima_mse_no_exog,\n",
        "        'SARIMA_RMSE_no_exog': sarima_rmse_no_exog,\n",
        "        'SARIMA_MAPE_no_exog': sarima_mape_no_exog\n",
        "    }\n",
        "\n",
        "    # Holt-Winters Exponential Smoothing Model\n",
        "    try:\n",
        "        hw_model = ExponentialSmoothing(\n",
        "            train_data[price_col],\n",
        "            trend='add',\n",
        "            seasonal='add',\n",
        "            seasonal_periods=5  # Adjust seasonal_periods as needed\n",
        "        ).fit()\n",
        "\n",
        "        hw_forecast = hw_model.forecast(steps=forecast_steps)\n",
        "        hw_forecast.index = test_data.index\n",
        "\n",
        "        # Reconstruct forecasted levels if differenced\n",
        "        if d_price > 0:\n",
        "            last_value = original_price_series.iloc[train_size - 1]\n",
        "            hw_forecast = hw_forecast.cumsum() + last_value\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error fitting Holt-Winters model for {bank_name}: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Calculate Holt-Winters accuracy metrics\n",
        "    hw_mae = np.mean(np.abs(actual - hw_forecast))\n",
        "    hw_mse = np.mean((actual - hw_forecast) ** 2)\n",
        "    hw_rmse = np.sqrt(hw_mse)\n",
        "    hw_mape = np.mean(np.abs((actual - hw_forecast) / actual)) * 100\n",
        "\n",
        "    hw_accuracy_metrics = {\n",
        "        'HoltWinters_MAE': hw_mae,\n",
        "        'HoltWinters_MSE': hw_mse,\n",
        "        'HoltWinters_RMSE': hw_rmse,\n",
        "        'HoltWinters_MAPE': hw_mape\n",
        "    }\n",
        "\n",
        "    metrics = {\n",
        "        'Bank': bank_name,\n",
        "        'Correlation': correlation,\n",
        "        'Min_Granger_pvalue': min(granger_pvalues.values()) if granger_pvalues else np.nan\n",
        "    }\n",
        "    metrics.update(arima_accuracy_metrics)\n",
        "    metrics.update(arima_accuracy_metrics_no_exog)\n",
        "    metrics.update(sarima_accuracy_metrics)\n",
        "    metrics.update(sarima_accuracy_metrics_no_exog)\n",
        "    metrics.update(hw_accuracy_metrics)\n",
        "\n",
        "    return {\n",
        "        'metrics': metrics,\n",
        "        'original_price_series': original_price_series,\n",
        "        'original_rate_series': original_rate_series,\n",
        "        'arima_forecast_mean': arima_forecast_mean,\n",
        "        'arima_conf_int': arima_conf_int,\n",
        "        'arima_forecast_mean_no_exog': arima_forecast_mean_no_exog,\n",
        "        'arima_conf_int_no_exog': arima_conf_int_no_exog,\n",
        "        'sarima_forecast_mean': sarima_forecast_mean,\n",
        "        'sarima_conf_int': sarima_conf_int,\n",
        "        'sarima_forecast_mean_no_exog': sarima_forecast_mean_no_exog,\n",
        "        'sarima_conf_int_no_exog': sarima_conf_int_no_exog,\n",
        "        'hw_forecast': hw_forecast,\n",
        "        'test_data': test_data\n",
        "    }\n",
        "\n",
        "# Create plots\n",
        "def plot_bank_analysis(bank_results):\n",
        "    sns.set_style('whitegrid')\n",
        "\n",
        "    fig = plt.figure(figsize=(20, 35))\n",
        "\n",
        "    # 1. Price and Interest Rate Time Series\n",
        "    ax1 = plt.subplot(711)\n",
        "    data_actual = bank_results['original_price_series']\n",
        "    ax1.plot(data_actual.index, data_actual, label=f'{bank_results[\"metrics\"][\"Bank\"]} Stock Price')\n",
        "    ax1.set_ylabel('Stock Price')\n",
        "    ax1.set_ylim([data_actual.min() * 0.9, data_actual.max() * 1.1])  # Adjust y-axis limits\n",
        "\n",
        "    ax2 = ax1.twinx()\n",
        "    ax2.plot(bank_results['original_rate_series'].index, bank_results['original_rate_series'],\n",
        "             color='red', alpha=0.5, label='Interest Rate')\n",
        "    ax2.set_ylabel('Interest Rate (%)', color='red')\n",
        "    ax2.set_ylim([bank_results['original_rate_series'].min() * 0.9, bank_results['original_rate_series'].max() * 1.1])\n",
        "\n",
        "    plt.title(f'{bank_results[\"metrics\"][\"Bank\"]}: Stock Price vs Interest Rate')\n",
        "    ax1.legend(loc='upper left')\n",
        "    ax2.legend(loc='upper right')\n",
        "\n",
        "    # 2. Scatter Plot with Trendline\n",
        "    plt.subplot(712)\n",
        "    x = bank_results['original_rate_series']\n",
        "    y = data_actual\n",
        "\n",
        "    plt.scatter(x, y, alpha=0.5, label='Data Points')\n",
        "\n",
        "    # Calculate the linear regression line\n",
        "    coefficients = np.polyfit(x, y, deg=1)\n",
        "    polynomial = np.poly1d(coefficients)\n",
        "    y_fit = polynomial(x)\n",
        "\n",
        "    # Plot the regression line\n",
        "    plt.plot(x, y_fit, color='red', label='Trend Line')\n",
        "\n",
        "    correlation = bank_results['metrics']['Correlation']\n",
        "    plt.xlabel('Interest Rate (%)')\n",
        "    plt.ylabel('Stock Price')\n",
        "    plt.title(f'Interest Rate vs Stock Price (Correlation: {correlation:.3f})')\n",
        "    plt.legend()\n",
        "\n",
        "    # 3. ARIMA Forecast with Exogenous Variables\n",
        "    plt.subplot(713)\n",
        "    arima_forecast_mean = bank_results['arima_forecast_mean']\n",
        "    arima_conf_int = bank_results['arima_conf_int']\n",
        "    test_data = bank_results['test_data']\n",
        "    actual = bank_results['original_price_series']\n",
        "\n",
        "    plt.plot(actual.index, actual, label='Actual')\n",
        "    plt.plot(arima_forecast_mean.index, arima_forecast_mean, '--', label='ARIMA Forecast with Exogenous')\n",
        "    plt.fill_between(arima_conf_int.index, arima_conf_int['lower'], arima_conf_int['upper'], color='gray', alpha=0.2, label='Confidence Interval')\n",
        "    plt.ylim([actual.min() * 0.9, actual.max() * 1.1])\n",
        "    plt.title(f'{bank_results[\"metrics\"][\"Bank\"]}: ARIMA Forecast with Exogenous Variables')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Stock Price')\n",
        "    plt.legend()\n",
        "\n",
        "    # 4. ARIMA Forecast without Exogenous Variables\n",
        "    plt.subplot(714)\n",
        "    arima_forecast_mean_no_exog = bank_results['arima_forecast_mean_no_exog']\n",
        "    arima_conf_int_no_exog = bank_results['arima_conf_int_no_exog']\n",
        "\n",
        "    plt.plot(actual.index, actual, label='Actual')\n",
        "    plt.plot(arima_forecast_mean_no_exog.index, arima_forecast_mean_no_exog, '--', label='ARIMA Forecast without Exogenous', color='green')\n",
        "    plt.fill_between(arima_conf_int_no_exog.index, arima_conf_int_no_exog['lower'], arima_conf_int_no_exog['upper'], color='gray', alpha=0.2, label='Confidence Interval')\n",
        "    plt.ylim([actual.min() * 0.9, actual.max() * 1.1])\n",
        "    plt.title(f'{bank_results[\"metrics\"][\"Bank\"]}: ARIMA Forecast without Exogenous Variables')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Stock Price')\n",
        "    plt.legend()\n",
        "\n",
        "    # 5. SARIMA Forecast with Exogenous Variables\n",
        "    plt.subplot(715)\n",
        "    sarima_forecast_mean = bank_results['sarima_forecast_mean']\n",
        "    sarima_conf_int = bank_results['sarima_conf_int']\n",
        "\n",
        "    plt.plot(actual.index, actual, label='Actual')\n",
        "    plt.plot(sarima_forecast_mean.index, sarima_forecast_mean, '--', label='SARIMA Forecast with Exogenous')\n",
        "    plt.fill_between(sarima_conf_int.index, sarima_conf_int['lower'], sarima_conf_int['upper'], color='gray', alpha=0.2, label='Confidence Interval')\n",
        "    plt.ylim([actual.min() * 0.9, actual.max() * 1.1])\n",
        "    plt.title(f'{bank_results[\"metrics\"][\"Bank\"]}: SARIMA Forecast with Exogenous Variables')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Stock Price')\n",
        "    plt.legend()\n",
        "\n",
        "    # 6. SARIMA Forecast without Exogenous Variables\n",
        "    plt.subplot(716)\n",
        "    sarima_forecast_mean_no_exog = bank_results['sarima_forecast_mean_no_exog']\n",
        "    sarima_conf_int_no_exog = bank_results['sarima_conf_int_no_exog']\n",
        "\n",
        "    plt.plot(actual.index, actual, label='Actual')\n",
        "    plt.plot(sarima_forecast_mean_no_exog.index, sarima_forecast_mean_no_exog, '--', label='SARIMA Forecast without Exogenous', color='green')\n",
        "    plt.fill_between(sarima_conf_int_no_exog.index, sarima_conf_int_no_exog['lower'], sarima_conf_int_no_exog['upper'], color='gray', alpha=0.2, label='Confidence Interval')\n",
        "    plt.ylim([actual.min() * 0.9, actual.max() * 1.1])\n",
        "    plt.title(f'{bank_results[\"metrics\"][\"Bank\"]}: SARIMA Forecast without Exogenous Variables')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Stock Price')\n",
        "    plt.legend()\n",
        "\n",
        "    # 7. Holt-Winters\n",
        "    plt.subplot(717)\n",
        "    hw_forecast = bank_results['hw_forecast']\n",
        "\n",
        "    plt.plot(actual.index, actual, label='Actual')\n",
        "    plt.plot(hw_forecast.index, hw_forecast, '--', label='Holt-Winters Forecast', color='purple')\n",
        "    plt.ylim([actual.min() * 0.9, actual.max() * 1.1])\n",
        "    plt.title(f'{bank_results[\"metrics\"][\"Bank\"]}: Holt-Winters Forecast')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Stock Price')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "def main():\n",
        "    bofa_file = 'BofA.csv'\n",
        "    gs_file = 'Goldman.csv'\n",
        "    jpm_file = 'JPM.csv'\n",
        "    rates_file = 'Rates.csv'\n",
        "\n",
        "    # Check if all files exist\n",
        "    for file in [bofa_file, gs_file, jpm_file, rates_file]:\n",
        "        if not os.path.exists(file):\n",
        "            print(f\"File not found: {file}\")\n",
        "            return\n",
        "\n",
        "    bofa_df = load_bank_data(bofa_file)\n",
        "    gs_df = load_bank_data(gs_file)\n",
        "    jpm_df = load_bank_data(jpm_file)\n",
        "    rates_df = load_rates_data(rates_file)\n",
        "\n",
        "    start_date = '2000-01-03'\n",
        "    if bofa_df is not None:\n",
        "        bofa_df = bofa_df[bofa_df.index >= start_date]\n",
        "    if gs_df is not None:\n",
        "        gs_df = gs_df[gs_df.index >= start_date]\n",
        "    if jpm_df is not None:\n",
        "        jpm_df = jpm_df[jpm_df.index >= start_date]\n",
        "    if rates_df is not None:\n",
        "        rates_df = rates_df[rates_df.index >= start_date]\n",
        "\n",
        "    banks = {\n",
        "        'Bank of America': bofa_df,\n",
        "        'Goldman Sachs': gs_df,\n",
        "        'JPMorgan Chase': jpm_df\n",
        "    }\n",
        "\n",
        "    metrics_summary = []\n",
        "    results = {}\n",
        "\n",
        "    for bank_name, bank_df in banks.items():\n",
        "        print(f\"\\n=== Analyzing {bank_name} ===\")\n",
        "        analysis_result = analyze_bank_rates_relationship(bank_df, rates_df, bank_name)\n",
        "\n",
        "        if analysis_result is None:\n",
        "            print(f\"Analysis failed for {bank_name}.\")\n",
        "            continue\n",
        "\n",
        "        # Extract and store metrics\n",
        "        metrics = analysis_result['metrics']\n",
        "        metrics_summary.append(metrics)\n",
        "\n",
        "        # Store data for plotting\n",
        "        results[bank_name] = analysis_result\n",
        "\n",
        "    if not metrics_summary:\n",
        "        print(\"No successful analyses to plot or summarize.\")\n",
        "        return\n",
        "\n",
        "    # Print Accuracy Metrics Grouped by Bank\n",
        "    for metrics in metrics_summary:\n",
        "        print(f\"\\n==== {metrics['Bank']} Accuracy Metrics ====\")\n",
        "        print(f\"Correlation with Interest Rates: {metrics['Correlation']:.6f}\")\n",
        "        if not np.isnan(metrics['Min_Granger_pvalue']):\n",
        "            print(f\"Minimum Granger Causality p-value: {metrics['Min_Granger_pvalue']:.6f}\")\n",
        "        else:\n",
        "            print(\"Granger Causality test was not performed due to insufficient data.\")\n",
        "\n",
        "        print(\"\\nARIMA Forecast Accuracy Metrics (with exogenous variables):\")\n",
        "        print(f\"  MAE: {metrics['ARIMA_MAE_with_exog']:.2f}\")\n",
        "        print(f\"  MSE: {metrics['ARIMA_MSE_with_exog']:.2f}\")\n",
        "        print(f\"  RMSE: {metrics['ARIMA_RMSE_with_exog']:.2f}\")\n",
        "        print(f\"  MAPE: {metrics['ARIMA_MAPE_with_exog']:.2f}%\")\n",
        "\n",
        "        print(\"\\nARIMA Forecast Accuracy Metrics (without exogenous variables):\")\n",
        "        print(f\"  MAE: {metrics['ARIMA_MAE_no_exog']:.2f}\")\n",
        "        print(f\"  MSE: {metrics['ARIMA_MSE_no_exog']:.2f}\")\n",
        "        print(f\"  RMSE: {metrics['ARIMA_RMSE_no_exog']:.2f}\")\n",
        "        print(f\"  MAPE: {metrics['ARIMA_MAPE_no_exog']:.2f}%\")\n",
        "\n",
        "        print(\"\\nSARIMA Forecast Accuracy Metrics (with exogenous variables):\")\n",
        "        print(f\"  MAE: {metrics['SARIMA_MAE_with_exog']:.2f}\")\n",
        "        print(f\"  MSE: {metrics['SARIMA_MSE_with_exog']:.2f}\")\n",
        "        print(f\"  RMSE: {metrics['SARIMA_RMSE_with_exog']:.2f}\")\n",
        "        print(f\"  MAPE: {metrics['SARIMA_MAPE_with_exog']:.2f}%\")\n",
        "\n",
        "        print(\"\\nSARIMA Forecast Accuracy Metrics (without exogenous variables):\")\n",
        "        print(f\"  MAE: {metrics['SARIMA_MAE_no_exog']:.2f}\")\n",
        "        print(f\"  MSE: {metrics['SARIMA_MSE_no_exog']:.2f}\")\n",
        "        print(f\"  RMSE: {metrics['SARIMA_RMSE_no_exog']:.2f}\")\n",
        "        print(f\"  MAPE: {metrics['SARIMA_MAPE_no_exog']:.2f}%\")\n",
        "\n",
        "        print(\"\\nHolt-Winters Forecast Accuracy Metrics:\")\n",
        "        print(f\"  MAE: {metrics['HoltWinters_MAE']:.2f}\")\n",
        "        print(f\"  MSE: {metrics['HoltWinters_MSE']:.2f}\")\n",
        "        print(f\"  RMSE: {metrics['HoltWinters_RMSE']:.2f}\")\n",
        "        print(f\"  MAPE: {metrics['HoltWinters_MAPE']:.2f}%\")\n",
        "\n",
        "    # Plotting for each bank\n",
        "    for bank_name, analysis_result in results.items():\n",
        "        print(f\"\\nGenerating plots for {bank_name}...\")\n",
        "        fig = plot_bank_analysis(analysis_result)\n",
        "        fig_path = f'{bank_name.replace(\" \", \"_\")}_analysis.png'\n",
        "        plt.savefig(fig_path)\n",
        "        plt.close()\n",
        "        print(f\"Plot saved to {fig_path}\")\n",
        "\n",
        "\n",
        "    return metrics_summary\n",
        "\n",
        "main()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5iSXEn85v6C",
        "outputId": "6d12c64a-7afc-4946-bf63-ed38d81c25a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully read BofA.csv\n",
            "Duplicate dates found in index. Handling duplicates...\n",
            "Successfully read Goldman.csv\n",
            "Some 'Pricing Date' values could not be converted to datetime.\n",
            "Successfully read JPM.csv\n",
            "Some 'Pricing Date' values could not be converted to datetime.\n",
            "Successfully read Rates.csv\n",
            "\n",
            "=== Analyzing Bank of America ===\n",
            "Date range of merged_df: 2000-01-03 00:00:00 to 2024-11-26 00:00:00\n",
            "Correlation between Share Price and MORTGAGE30US: 0.43030396341548355\n",
            "\n",
            "=== Stationarity Test for Bank of America Price ===\n",
            "ADF Statistic: -1.251355864919427\n",
            "p-value: 0.6511231082286751\n",
            "Critical values:\n",
            "\t1%: -3.431360019071387\n",
            "\t5%: -2.8619863416168188\n",
            "\t10%: -2.567007584327379\n",
            "Bank of America Price is non-stationary. Differencing is recommended.\n",
            "\n",
            "=== Stationarity Test for Interest Rate ===\n",
            "ADF Statistic: -2.1577118851367367\n",
            "p-value: 0.22194985197955258\n",
            "Critical values:\n",
            "\t1%: -3.4313625212742105\n",
            "\t5%: -2.8619874471870737\n",
            "\t10%: -2.567008172846683\n",
            "Interest Rate is non-stationary. Differencing is recommended.\n",
            "\n",
            "=== Stationarity Test for Bank of America Price After Differencing ===\n",
            "ADF Statistic: -20.07614938394006\n",
            "p-value: 0.0\n",
            "Critical values:\n",
            "\t1%: -3.431360019071387\n",
            "\t5%: -2.8619863416168188\n",
            "\t10%: -2.567007584327379\n",
            "Bank of America Price After Differencing is stationary.\n",
            "\n",
            "=== Stationarity Test for Interest Rate After Differencing ===\n",
            "ADF Statistic: -13.935255237486968\n",
            "p-value: 5.004692537970243e-26\n",
            "Critical values:\n",
            "\t1%: -3.4313625212742105\n",
            "\t5%: -2.8619874471870737\n",
            "\t10%: -2.567008172846683\n",
            "Interest Rate After Differencing is stationary.\n",
            "\n",
            "=== Analyzing Goldman Sachs ===\n",
            "Date range of merged_df: 2000-01-03 00:00:00 to 2024-11-27 00:00:00\n",
            "Correlation between Share Price and MORTGAGE30US: -0.13343506199207392\n",
            "\n",
            "=== Stationarity Test for Goldman Sachs Price ===\n",
            "ADF Statistic: 1.3882830332446074\n",
            "p-value: 0.997060258129785\n",
            "Critical values:\n",
            "\t1%: -3.4313579951196527\n",
            "\t5%: -2.8619854473557638\n",
            "\t10%: -2.5670071082927155\n",
            "Goldman Sachs Price is non-stationary. Differencing is recommended.\n",
            "\n",
            "=== Stationarity Test for Interest Rate ===\n",
            "ADF Statistic: -2.1661722427679755\n",
            "p-value: 0.21875119148377137\n",
            "Critical values:\n",
            "\t1%: -3.4313623645233724\n",
            "\t5%: -2.8619873779285014\n",
            "\t10%: -2.567008135978821\n",
            "Interest Rate is non-stationary. Differencing is recommended.\n",
            "\n",
            "=== Stationarity Test for Goldman Sachs Price After Differencing ===\n",
            "ADF Statistic: -30.731227547466542\n",
            "p-value: 0.0\n",
            "Critical values:\n",
            "\t1%: -3.4313579951196527\n",
            "\t5%: -2.8619854473557638\n",
            "\t10%: -2.5670071082927155\n",
            "Goldman Sachs Price After Differencing is stationary.\n",
            "\n",
            "=== Stationarity Test for Interest Rate After Differencing ===\n",
            "ADF Statistic: -13.961157417004557\n",
            "p-value: 4.517413409944686e-26\n",
            "Critical values:\n",
            "\t1%: -3.4313623645233724\n",
            "\t5%: -2.8619873779285014\n",
            "\t10%: -2.567008135978821\n",
            "Interest Rate After Differencing is stationary.\n",
            "\n",
            "=== Analyzing JPMorgan Chase ===\n",
            "Date range of merged_df: 2000-01-03 00:00:00 to 2024-11-27 00:00:00\n",
            "Correlation between Share Price and MORTGAGE30US: -0.15302632097447125\n",
            "\n",
            "=== Stationarity Test for JPMorgan Chase Price ===\n",
            "ADF Statistic: 2.2245387336553706\n",
            "p-value: 0.9989020960330874\n",
            "Critical values:\n",
            "\t1%: -3.431359239667287\n",
            "\t5%: -2.8619859972456685\n",
            "\t10%: -2.5670074010110926\n",
            "JPMorgan Chase Price is non-stationary. Differencing is recommended.\n",
            "\n",
            "=== Stationarity Test for Interest Rate ===\n",
            "ADF Statistic: -2.1661722427679755\n",
            "p-value: 0.21875119148377137\n",
            "Critical values:\n",
            "\t1%: -3.4313623645233724\n",
            "\t5%: -2.8619873779285014\n",
            "\t10%: -2.567008135978821\n",
            "Interest Rate is non-stationary. Differencing is recommended.\n",
            "\n",
            "=== Stationarity Test for JPMorgan Chase Price After Differencing ===\n",
            "ADF Statistic: -22.24630457495884\n",
            "p-value: 0.0\n",
            "Critical values:\n",
            "\t1%: -3.431359239667287\n",
            "\t5%: -2.8619859972456685\n",
            "\t10%: -2.5670074010110926\n",
            "JPMorgan Chase Price After Differencing is stationary.\n",
            "\n",
            "=== Stationarity Test for Interest Rate After Differencing ===\n",
            "ADF Statistic: -13.961157417004557\n",
            "p-value: 4.517413409944686e-26\n",
            "Critical values:\n",
            "\t1%: -3.4313623645233724\n",
            "\t5%: -2.8619873779285014\n",
            "\t10%: -2.567008135978821\n",
            "Interest Rate After Differencing is stationary.\n",
            "\n",
            "==== Bank of America Accuracy Metrics ====\n",
            "Correlation with Interest Rates: 0.430304\n",
            "Minimum Granger Causality p-value: 0.930022\n",
            "\n",
            "ARIMA Forecast Accuracy Metrics (with exogenous variables):\n",
            "  MAE: 5.51\n",
            "  MSE: 43.65\n",
            "  RMSE: 6.61\n",
            "  MAPE: 17.17%\n",
            "\n",
            "ARIMA Forecast Accuracy Metrics (without exogenous variables):\n",
            "  MAE: 6.14\n",
            "  MSE: 52.06\n",
            "  RMSE: 7.22\n",
            "  MAPE: 20.00%\n",
            "\n",
            "SARIMA Forecast Accuracy Metrics (with exogenous variables):\n",
            "  MAE: 5.58\n",
            "  MSE: 44.62\n",
            "  RMSE: 6.68\n",
            "  MAPE: 17.57%\n",
            "\n",
            "SARIMA Forecast Accuracy Metrics (without exogenous variables):\n",
            "  MAE: 5.44\n",
            "  MSE: 42.71\n",
            "  RMSE: 6.54\n",
            "  MAPE: 17.00%\n",
            "\n",
            "Holt-Winters Forecast Accuracy Metrics:\n",
            "  MAE: 5.55\n",
            "  MSE: 44.40\n",
            "  RMSE: 6.66\n",
            "  MAPE: 17.80%\n",
            "\n",
            "==== Goldman Sachs Accuracy Metrics ====\n",
            "Correlation with Interest Rates: -0.133435\n",
            "Minimum Granger Causality p-value: 0.915335\n",
            "\n",
            "ARIMA Forecast Accuracy Metrics (with exogenous variables):\n",
            "  MAE: 159.03\n",
            "  MSE: 35306.94\n",
            "  RMSE: 187.90\n",
            "  MAPE: 38.17%\n",
            "\n",
            "ARIMA Forecast Accuracy Metrics (without exogenous variables):\n",
            "  MAE: 162.15\n",
            "  MSE: 36544.90\n",
            "  RMSE: 191.17\n",
            "  MAPE: 38.94%\n",
            "\n",
            "SARIMA Forecast Accuracy Metrics (with exogenous variables):\n",
            "  MAE: 160.56\n",
            "  MSE: 35868.79\n",
            "  RMSE: 189.39\n",
            "  MAPE: 38.59%\n",
            "\n",
            "SARIMA Forecast Accuracy Metrics (without exogenous variables):\n",
            "  MAE: 160.88\n",
            "  MSE: 36001.71\n",
            "  RMSE: 189.74\n",
            "  MAPE: 38.66%\n",
            "\n",
            "Holt-Winters Forecast Accuracy Metrics:\n",
            "  MAE: 134.26\n",
            "  MSE: 25820.99\n",
            "  RMSE: 160.69\n",
            "  MAPE: 32.65%\n",
            "\n",
            "==== JPMorgan Chase Accuracy Metrics ====\n",
            "Correlation with Interest Rates: -0.153026\n",
            "Minimum Granger Causality p-value: 0.918473\n",
            "\n",
            "ARIMA Forecast Accuracy Metrics (with exogenous variables):\n",
            "  MAE: 194.33\n",
            "  MSE: 50732.39\n",
            "  RMSE: 225.24\n",
            "  MAPE: 19.54%\n",
            "\n",
            "ARIMA Forecast Accuracy Metrics (without exogenous variables):\n",
            "  MAE: 203.17\n",
            "  MSE: 57709.18\n",
            "  RMSE: 240.23\n",
            "  MAPE: 21.28%\n",
            "\n",
            "SARIMA Forecast Accuracy Metrics (with exogenous variables):\n",
            "  MAE: 195.35\n",
            "  MSE: 51170.71\n",
            "  RMSE: 226.21\n",
            "  MAPE: 19.79%\n",
            "\n",
            "SARIMA Forecast Accuracy Metrics (without exogenous variables):\n",
            "  MAE: 193.74\n",
            "  MSE: 50647.76\n",
            "  RMSE: 225.05\n",
            "  MAPE: 19.54%\n",
            "\n",
            "Holt-Winters Forecast Accuracy Metrics:\n",
            "  MAE: 6202.14\n",
            "  MSE: 62380079.22\n",
            "  RMSE: 7898.11\n",
            "  MAPE: 513.55%\n",
            "\n",
            "Generating plots for Bank of America...\n",
            "Plot saved to Bank_of_America_analysis.png\n",
            "\n",
            "Generating plots for Goldman Sachs...\n",
            "Plot saved to Goldman_Sachs_analysis.png\n",
            "\n",
            "Generating plots for JPMorgan Chase...\n",
            "Plot saved to JPMorgan_Chase_analysis.png\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'Bank': 'Bank of America',\n",
              "  'Correlation': 0.43030396341548355,\n",
              "  'Min_Granger_pvalue': 0.93002162615197,\n",
              "  'ARIMA_MAE_with_exog': 5.508448563437514,\n",
              "  'ARIMA_MSE_with_exog': 43.6516929220888,\n",
              "  'ARIMA_RMSE_with_exog': 6.6069427818083,\n",
              "  'ARIMA_MAPE_with_exog': 17.16884212900028,\n",
              "  'ARIMA_MAE_no_exog': 6.135168407512112,\n",
              "  'ARIMA_MSE_no_exog': 52.063804033351126,\n",
              "  'ARIMA_RMSE_no_exog': 7.21552520842046,\n",
              "  'ARIMA_MAPE_no_exog': 19.997500791521382,\n",
              "  'SARIMA_MAE_with_exog': 5.57609241143935,\n",
              "  'SARIMA_MSE_with_exog': 44.61720311258997,\n",
              "  'SARIMA_RMSE_with_exog': 6.679611000094988,\n",
              "  'SARIMA_MAPE_with_exog': 17.566810030567556,\n",
              "  'SARIMA_MAE_no_exog': 5.443962674596028,\n",
              "  'SARIMA_MSE_no_exog': 42.71276426435557,\n",
              "  'SARIMA_RMSE_no_exog': 6.535500307119231,\n",
              "  'SARIMA_MAPE_no_exog': 16.995515269550136,\n",
              "  'HoltWinters_MAE': 5.545339631051432,\n",
              "  'HoltWinters_MSE': 44.400231230560216,\n",
              "  'HoltWinters_RMSE': 6.663349850530153,\n",
              "  'HoltWinters_MAPE': 17.79611205011175},\n",
              " {'Bank': 'Goldman Sachs',\n",
              "  'Correlation': -0.13343506199207392,\n",
              "  'Min_Granger_pvalue': 0.9153349773742472,\n",
              "  'ARIMA_MAE_with_exog': 159.02587623463108,\n",
              "  'ARIMA_MSE_with_exog': 35306.935415215055,\n",
              "  'ARIMA_RMSE_with_exog': 187.90139811937286,\n",
              "  'ARIMA_MAPE_with_exog': 38.17021472213612,\n",
              "  'ARIMA_MAE_no_exog': 162.1469410584022,\n",
              "  'ARIMA_MSE_no_exog': 36544.89711767027,\n",
              "  'ARIMA_RMSE_no_exog': 191.16719676155287,\n",
              "  'ARIMA_MAPE_no_exog': 38.93719046540311,\n",
              "  'SARIMA_MAE_with_exog': 160.56242892996383,\n",
              "  'SARIMA_MSE_with_exog': 35868.78903218848,\n",
              "  'SARIMA_RMSE_with_exog': 189.39057271202407,\n",
              "  'SARIMA_MAPE_with_exog': 38.58713046475963,\n",
              "  'SARIMA_MAE_no_exog': 160.8794459096846,\n",
              "  'SARIMA_MSE_no_exog': 36001.71090220556,\n",
              "  'SARIMA_RMSE_no_exog': 189.74116817972202,\n",
              "  'SARIMA_MAPE_no_exog': 38.6588971624219,\n",
              "  'HoltWinters_MAE': 134.25727677580292,\n",
              "  'HoltWinters_MSE': 25820.992739294306,\n",
              "  'HoltWinters_RMSE': 160.68911829770647,\n",
              "  'HoltWinters_MAPE': 32.65366737441229},\n",
              " {'Bank': 'JPMorgan Chase',\n",
              "  'Correlation': -0.15302632097447125,\n",
              "  'Min_Granger_pvalue': 0.9184728901853473,\n",
              "  'ARIMA_MAE_with_exog': 194.33020410703904,\n",
              "  'ARIMA_MSE_with_exog': 50732.39296083837,\n",
              "  'ARIMA_RMSE_with_exog': 225.2385245930153,\n",
              "  'ARIMA_MAPE_with_exog': 19.544782295110256,\n",
              "  'ARIMA_MAE_no_exog': 203.17091292066135,\n",
              "  'ARIMA_MSE_no_exog': 57709.18031613561,\n",
              "  'ARIMA_RMSE_no_exog': 240.22735130733054,\n",
              "  'ARIMA_MAPE_no_exog': 21.28279858738097,\n",
              "  'SARIMA_MAE_with_exog': 195.35370679288673,\n",
              "  'SARIMA_MSE_with_exog': 51170.706459314846,\n",
              "  'SARIMA_RMSE_with_exog': 226.2094305269231,\n",
              "  'SARIMA_MAPE_with_exog': 19.790601163641174,\n",
              "  'SARIMA_MAE_no_exog': 193.74420110346793,\n",
              "  'SARIMA_MSE_no_exog': 50647.75933442464,\n",
              "  'SARIMA_RMSE_no_exog': 225.05057061563883,\n",
              "  'SARIMA_MAPE_no_exog': 19.542568111799326,\n",
              "  'HoltWinters_MAE': 6202.138962132782,\n",
              "  'HoltWinters_MSE': 62380079.218818925,\n",
              "  'HoltWinters_RMSE': 7898.106052644452,\n",
              "  'HoltWinters_MAPE': 513.5502657234989}]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}